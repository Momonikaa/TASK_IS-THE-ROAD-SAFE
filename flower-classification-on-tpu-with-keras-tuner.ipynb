{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q tensorflow==2.3.0 # Use 2.3.0 for built-in EfficientNet\n","!pip install -q git+https://github.com/keras-team/keras-tuner@master # Use github head for newly added TPU support\n","!pip install -q cloud-tpu-client # Needed for sync TPU version"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import random, re, math\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf, tensorflow.keras.backend as K\n","from kaggle_datasets import KaggleDatasets\n","print('Tensorflow version ' + tf.__version__)\n","import kerastuner as kt"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Detect hardware, return appropriate distribution strategy\n","try:\n","    # Sync TPU version\n","    from cloud_tpu_client import Client\n","    c = Client()\n","    c.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n","    \n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","    \n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.data.experimental import AUTOTUNE\n","\n","# Configuration\n","IMAGE_SIZE = [331, 331]\n","EPOCHS_SEARCH = 10\n","EPOCHS_FINAL = 20\n","SEED = 123\n","BATCH_SIZE = 32 * strategy.num_replicas_in_sync"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data access\n","GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\n","\n","GCS_PATH_SELECT = { # available image sizes\n","    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n","    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n","    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n","    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n","}\n","\n","GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n","\n","TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n","VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n","TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n","           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n","           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n","           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n","           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n","           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n","           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n","           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n","           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n","           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n","           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def decode_image(image_data):\n","    image = tf.image.decode_jpeg(image_data, channels=3)\n","    image = tf.cast(image, tf.float32) \n","    # For keras.application implementation of EfficientNet, input should be [0, 255]\n","    image = tf.ensure_shape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n","    return image\n","\n","def read_labeled_tfrecord(example):\n","    LABELED_TFREC_FORMAT = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n","        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n","    }\n","    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    label = tf.cast(example['class'], tf.int32)\n","    return image, label # returns a dataset of (image, label) pairs\n","\n","def read_unlabeled_tfrecord(example):\n","    UNLABELED_TFREC_FORMAT = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n","        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n","        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n","    }\n","    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    idnum = example['id']\n","    return image, idnum # returns a dataset of image(s)\n","\n","def load_dataset(filenames, labeled = True, ordered = False):\n","    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n","    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n","    \n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False # disable order, increase speed\n","\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTOTUNE) # automatically interleaves reads from multiple files\n","    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n","    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord,\n","                          num_parallel_calls = AUTOTUNE) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n","    return dataset\n","\n","\n","def one_hot_encoding(image, label, num_classes=len(CLASSES)):\n","    return image, tf.one_hot(label, num_classes)\n","\n","def get_training_dataset(dataset,do_aug=True):\n","    dataset = dataset.map(one_hot_encoding, num_parallel_calls=AUTOTUNE)\n","    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n","    dataset = dataset.shuffle(2048)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # Drop remainder to ensure same batch size for all.\n","    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def get_validation_dataset(dataset):\n","    dataset = dataset.map(one_hot_encoding, num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    dataset = dataset.cache()\n","    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def get_test_dataset(ordered=False):\n","    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n","    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def count_data_items(filenames):\n","    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n","    return np.sum(n)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n","    # returns 3x3 transformmatrix which transforms indicies\n","        \n","    # CONVERT DEGREES TO RADIANS\n","    rotation = math.pi * rotation / 180.\n","    shear = math.pi * shear / 180.\n","    \n","    # ROTATION MATRIX\n","    c1 = tf.math.cos(rotation)\n","    s1 = tf.math.sin(rotation)\n","    one = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n","        \n","    # SHEAR MATRIX\n","    c2 = tf.math.cos(shear)\n","    s2 = tf.math.sin(shear)\n","    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n","    \n","    # ZOOM MATRIX\n","    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n","    \n","    # SHIFT MATRIX\n","    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n","    \n","    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def transform(image,label):\n","    image = tf.image.random_flip_left_right(image)\n","    \n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    # output - image randomly rotated, sheared, zoomed, and shifted\n","    DIM = IMAGE_SIZE[0]\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    rot = 15. * tf.random.normal([1],dtype='float32')\n","    shr = 5. * tf.random.normal([1],dtype='float32') \n","    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n","    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n","    h_shift = 16. * tf.random.normal([1],dtype='float32') \n","    w_shift = 16. * tf.random.normal([1],dtype='float32') \n","  \n","    # GET TRANSFORMATION MATRIX\n","    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n","\n","    # LIST DESTINATION PIXEL INDICES\n","    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n","    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n","    z = tf.ones([DIM*DIM],dtype='int32')\n","    idx = tf.stack( [x,y,z] )\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n","    idx2 = K.cast(idx2,dtype='int32')\n","    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES           \n","    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n","    d = tf.gather_nd(image,tf.transpose(idx3))\n","    \n","    return tf.reshape(d,[DIM,DIM,3]),label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["row = 3; col = 4;\n","all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n","one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\n","augmented_element = one_element.repeat().map(transform).batch(row*col)\n","\n","for (img,label) in augmented_element:\n","    plt.figure(figsize=(15,int(15*row/col)))\n","    for j in range(row*col):\n","        plt.subplot(row,col,j+1)\n","        plt.axis('off')\n","        plt.imshow(img[j,] / 255.)\n","    plt.show()\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# Search Hyper-parameters with Keras-Tuner\n","\n","Now we search hyperparameters with Keras-Tuner.\n","\n","A HyperModel in Keras-Tuner is class with a `build` method that creates a *compiled* Keras model using a set of hyperparameters for each trial. A tuner takes a [HyperModel](https://keras-team.github.io/keras-tuner/documentation/hypermodels/) or simply a model builder function, and tries the combinations of the hyperparameters for times depending on different tuning algorithms (defined by [Oracle](https://keras-team.github.io/keras-tuner/documentation/oracles/)). Each of the built-in [Tuner](https://keras-team.github.io/keras-tuner/documentation/tuners/) have corresponding oracle.\n","\n","In this example I only use pre-built HyperModel and Tuner. It is also possible to create any HyperModel or model building function, and to create custom tuning algorithms by subclassing Oracles, and to use custom training loop by [subclassing Tuner](https://keras-team.github.io/keras-tuner/tutorials/subclass-tuner/).\n","\n","A side note: TF2.3 provides `experimental_steps_per_execution` keyword for `model.compile`. This greatly improves TPU efficiency. To use the feature in Keras Tuner, you will need to modify the model building function:\n","```python\n","class MyHyperEfficientNet(HyperEfficientNet):\n","    def _compile(self, model, hp):\n","        super(MyHyperEfficientNet, self)._compile(model, hp)\n","        model.compile(\n","            optimizer=model.optimizer,\n","            loss='categorical_crossentropy',\n","            metrics=['accuracy'],\n","            experimental_steps_per_execution=4)\n","```\n","and then use the new subclass for HyperModel."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define HyperModel using built-in application\n","from kerastuner.applications.efficientnet import HyperEfficientNet\n","hm = HyperEfficientNet(input_shape=[IMAGE_SIZE[0], IMAGE_SIZE[1], 3] , classes=len(CLASSES))\n","\n","# Optional: Restrict default hyperparameters.\n","# To take effect, pass this `hp` instance when constructing tuner as `hyperparameters=hp`\n","from kerastuner.engine.hyperparameters import HyperParameters\n","hp = HyperParameters()\n","hp.Choice('version', ['B0', 'B1', 'B2', 'B3', 'B4']) #restrict choice of EfficientNet version from B0-B7 to B0-B4\n","\n","# Initiate Tuner\n","tuner = kt.tuners.randomsearch.RandomSearch(\n","    hypermodel=hm,\n","    objective='val_accuracy',\n","    max_trials=5,\n","    distribution_strategy=strategy, # This strategy's scope is used for building each model during the search.\n","    directory='flower_classification',\n","    project_name='randomsearch_efficientnet',\n","    hyperparameters=hp,\n",")\n","tuner.search_space_summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_val_samples = count_data_items(VALIDATION_FILENAMES)\n","num_train_samples = count_data_items(TRAINING_FILENAMES)\n","\n","train_ds = get_training_dataset(load_dataset(TRAINING_FILENAMES))\n","validation_ds = get_validation_dataset(load_dataset(VALIDATION_FILENAMES))\n","num_train_batches = num_train_samples // BATCH_SIZE\n","num_val_batches = num_val_samples // BATCH_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tuner.search(train_ds,\n","             epochs=EPOCHS_SEARCH,\n","             validation_data=validation_ds,\n","             steps_per_epoch=num_train_batches,\n","             verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tuner.results_summary()\n","model = tuner.get_best_models()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ds_all =  get_training_dataset(load_dataset(TRAINING_FILENAMES + VALIDATION_FILENAMES))\n","\n","# Train the best model with all data\n","model.fit(ds_all,\n","          epochs=EPOCHS_FINAL,\n","          steps_per_epoch=num_train_batches + num_val_batches,\n","          callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],\n","          verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ds_test = get_test_dataset(ordered=True)\n","\n","print('Computing predictions...')\n","predictions = []\n","\n","for i, (test_img, test_id) in enumerate(ds_test):\n","    print('Processing batch ', i)\n","    probabilities = model(test_img)\n","    prediction = np.argmax(probabilities, axis=-1)\n","    predictions.append(prediction)\n","\n","predictions = np.concatenate(predictions)\n","print('Number of test examples predicted: ', predictions.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get image ids from test set and convert to unicode\n","ds_test_ids = ds_test.map(lambda image, idnum: idnum).unbatch()\n","test_ids = next(iter(ds_test_ids.batch(np.iinfo(np.int64).max))).numpy().astype('U')\n","\n","# Write the submission file\n","np.savetxt(\n","    'submission.csv',\n","    np.rec.fromarrays([test_ids, predictions]),\n","    fmt=['%s', '%d'],\n","    delimiter=',',\n","    header='id,label',\n","    comments='',\n",")\n","\n","# Look at the first few predictions\n","!head submission.csv"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
